{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "p = 1    # number of features\n",
    "\n",
    "def build_linear_dataset(N, noise_std=0.1):\n",
    "    X = np.linspace(-6, 6, num=N)\n",
    "    y = 3 * X + 1 + np.random.normal(0, noise_std, size=N)\n",
    "    X, y = X.reshape((N, 1)), y.reshape((N, 1))\n",
    "    X, y = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "    return torch.cat((X, y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.01)\n",
    "num_iterations = 500\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N, p)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data)\n",
    "        # calculate the mse_loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.data[0]))\n",
    "    \n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" % (name, param.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 11412.3311\n",
      "[iteration 0100] loss: 8260.8232\n",
      "[iteration 0150] loss: 5834.0771\n",
      "[iteration 0200] loss: 4014.7400\n",
      "[iteration 0250] loss: 2690.4233\n",
      "[iteration 0300] loss: 1756.9719\n",
      "[iteration 0350] loss: 1121.4536\n",
      "[iteration 0400] loss: 704.4464\n",
      "[iteration 0450] loss: 441.2144\n",
      "[iteration 0500] loss: 281.5907\n",
      "Learned parameters:\n",
      "linear.weight: 2.638\n",
      "linear.bias: 1.002\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(1, 1))\n",
    "sigma = Variable(torch.ones(1, 1))\n",
    "\n",
    "# define a unit normal prior\n",
    "prior = Normal(mu, sigma)\n",
    "# overload the parameters in the regression module with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a regressor from the prior\n",
    "sampled_reg_model = lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel (\n",
       "  (linear): Linear (1 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    \n",
    "    # Define priors\n",
    "    mu, sigma = Variable(torch.zeros(p, 1)), Variable(10 * torch.ones(p, 1))\n",
    "    bias_mu, bias_sigma = Variable(torch.zeros(1)), Variable(10 * torch.ones(1))\n",
    "    w_prior, b_prior = Normal(mu, sigma), Normal(bias_mu, bias_sigma)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    # run the regressor forward conditioned on data\n",
    "    prediction_mean = lifted_reg_model(x_data).squeeze()\n",
    "    # condition on the observed data\n",
    "    pyro.observe(\"obs\", Normal(prediction_mean, Variable(0.1 * torch.ones(data.size(0)))), y_data.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    # define our variational parameters\n",
    "    w_mu = Variable(torch.randn(p, 1), requires_grad=True)\n",
    "    # note that we initialize our sigmas to be pretty narrow\n",
    "    w_log_sig = Variable(-3.0 * torch.ones(p, 1) + 0.05 * torch.randn(p, 1), requires_grad=True)\n",
    "    b_mu = Variable(torch.randn(1), requires_grad=True)\n",
    "    b_log_sig = Variable(-3.0 * torch.ones(1) + 0.05 * torch.randn(1), requires_grad=True)\n",
    "    \n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_mu)\n",
    "    sw_param = softplus(pyro.param(\"guide_log_sigma_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_mu)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_sigma_bias\", b_log_sig))\n",
    "    # guide distributions for w and b\n",
    "    w_dist, b_dist = Normal(mw_param, sw_param), Normal(mb_param, sb_param)\n",
    "    dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=\"ELBO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 4171.7150\n",
      "[iteration 0101] loss: 1850.1337\n",
      "[iteration 0201] loss: 727.9838\n",
      "[iteration 0301] loss: 172.5466\n",
      "[iteration 0401] loss: 108.6810\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "data = build_linear_dataset(N, p)\n",
    "for j in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(data)\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / float(N)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guide_log_sigma_weight]: -3.003\n",
      "[guide_log_sigma_bias]: -3.173\n",
      "[guide_mean_weight]: 2.923\n",
      "[guide_mean_bias]: 0.915\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"[%s]: %.3f\" % (name, pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss: ', 0.33393630385398865)\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(6, 7, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = Variable(torch.Tensor(X)), Variable(torch.Tensor(y))\n",
    "loss = nn.MSELoss()\n",
    "y_preds = Variable(torch.zeros(20, 1))\n",
    "for i in range(20):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / 20\n",
    "print(\"Loss: \", loss(y_preds, y_data).data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
